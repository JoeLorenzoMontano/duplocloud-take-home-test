# Ollama Model Settings
MODEL=llama2:latest
EMBEDDING_MODEL=all-minilm:l6-v2

# Host Ollama Integration (uncomment to use host machine's Ollama)
# USE_HOST_OLLAMA=true                    # Set to true to use host Ollama instead of container
# HOST_OLLAMA=http://host.docker.internal:11434  # URL for host machine's Ollama (works on Docker Desktop)
# HOST_NETWORK=host                       # Uncomment to use host networking (Linux only)
# OLLAMA_PROFILE=noollama                 # Docker compose profile that excludes the Ollama container

# GPU Acceleration Settings (for containerized Ollama)
OLLAMA_GPU_DEVICES=all
OLLAMA_GPU_COUNT=1
OLLAMA_GPU_MODE=shared
OLLAMA_GPU_LAYERS=32

# Web Search Integration
SERPER_API_KEY=your_serper_dev_api_key_here