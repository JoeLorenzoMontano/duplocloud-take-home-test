# Ollama Model Settings
MODEL=llama2:latest
EMBEDDING_MODEL=all-minilm:l6-v2

# Host Ollama Integration (uncomment to use host machine's Ollama)
# HOST_OLLAMA=http://host.docker.internal:11434  # URL for host's Ollama (macOS/Windows)
# HOST_OLLAMA=http://localhost:11434             # URL for host's Ollama (Linux)

# GPU Acceleration Settings (for containerized Ollama)
OLLAMA_GPU_DEVICES=all
OLLAMA_GPU_COUNT=1
OLLAMA_GPU_MODE=shared
OLLAMA_GPU_LAYERS=32

# Web Search Integration
SERPER_API_KEY=your_serper_dev_api_key_here