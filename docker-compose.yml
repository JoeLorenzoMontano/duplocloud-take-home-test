services:
  fastapi-app:
    build: ./app
    container_name: document-processing-service
    restart: always
    ports:
      - "8000:8000"
    depends_on:
      - chromadb
      - ollama
    volumes:
      - ./rag-documents:/app/data
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - CHROMA_HOST=chromadb
      - CHROMA_PORT=8000
      - MODEL=llama2:latest
      - EMBEDDING_MODEL=all-minilm:l6-v2
      # Document chunking settings
      - ENABLE_CHUNKING=true
      - MAX_CHUNK_SIZE=1000
      - MIN_CHUNK_SIZE=200
      - CHUNK_OVERLAP=100
    networks:
      - app-network

  chromadb:
    image: ghcr.io/chroma-core/chroma:latest
    container_name: chromadb
    restart: always
    volumes:
      - chromadb_data:/chroma/chroma
    ports:
      - "8001:8000"
    environment:
      - IS_PERSISTENT=TRUE
      - ALLOW_RESET=TRUE
      - ANONYMIZED_TELEMETRY=FALSE
      - CHROMA_SERVER_NOFILE=65535
      # Pin NumPy to avoid compatibility issues
      - NUMPY_VERSION=1.26.0
    networks:
      - app-network

  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    restart: always
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_MODELS=/root/.ollama/models
      - CUDA_VISIBLE_DEVICES=${OLLAMA_GPU_DEVICES:-""}
      - OLLAMA_GPU_LAYERS=${OLLAMA_GPU_LAYERS:-0}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${OLLAMA_GPU_COUNT:-0}
              capabilities: [gpu]
              options:
                mode: ${OLLAMA_GPU_MODE:-""}
    networks:
      - app-network

volumes:
  chromadb_data:
  ollama_data:

networks:
  app-network:
    driver: bridge
