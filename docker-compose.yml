services:
  api:
    build: ./app
    container_name: document-processing-service
    restart: always
    ports:
      - "8000:8000"
    depends_on:
      - chromadb
      - ollama
    volumes:
      - ./rag-documents:/app/data
    environment:
      # Ollama configuration - use HOST_OLLAMA to override and use host machine Ollama
      - OLLAMA_BASE_URL=${HOST_OLLAMA:-http://ollama:11434}
      - CHROMA_HOST=chromadb
      - CHROMA_PORT=8000
      - MODEL=${MODEL}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL}
      # Document chunking settings
      - ENABLE_CHUNKING=true
      - MAX_CHUNK_SIZE=1000
      - MIN_CHUNK_SIZE=200
      - CHUNK_OVERLAP=100
      # Web search settings
      - SERPER_API_KEY=${SERPER_API_KEY}
    networks:
      - app-network
      ${HOST_NETWORK:-}
      
  ui:
    build: ./ui
    container_name: document-ui-service
    restart: always
    ports:
      - "5000:5000"
    depends_on:
      - api
    environment:
      - API_URL=http://api:8000
    networks:
      - app-network
      ${HOST_NETWORK:-}

  chromadb:
    image: ghcr.io/chroma-core/chroma:latest
    container_name: chromadb
    restart: always
    volumes:
      - chromadb_data:/chroma/chroma
    ports:
      - "8001:8000"
    environment:
      - IS_PERSISTENT=TRUE
      - ALLOW_RESET=TRUE
      - ANONYMIZED_TELEMETRY=FALSE
      - CHROMA_SERVER_NOFILE=65535
      # Pin NumPy to avoid compatibility issues
      - NUMPY_VERSION=1.26.0
    networks:
      - app-network

  # Ollama container (optional - can be disabled with USE_HOST_OLLAMA=true)
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    restart: always
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_MODELS=/root/.ollama/models
      - CUDA_VISIBLE_DEVICES=${OLLAMA_GPU_DEVICES:-""}
      - OLLAMA_GPU_LAYERS=${OLLAMA_GPU_LAYERS:-0}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${OLLAMA_GPU_COUNT:-0}
              capabilities: [gpu]
              options:
                mode: ${OLLAMA_GPU_MODE:-""}
    networks:
      - app-network
    profiles:
      - ${OLLAMA_PROFILE:-default}

volumes:
  chromadb_data:
  ollama_data:

networks:
  app-network:
    driver: bridge
